import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import pandas as pd
import time
import math
import os
import random
import argparse
from sentence_transformers import CrossEncoder, InputExample
from torch.utils.data import DataLoader, DistributedSampler
from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator

def setup_ddp():
    dist.init_process_group(backend='nccl')
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    return rank, world_size, local_rank

def cleanup_ddp():
    dist.destroy_process_group()

def add_noise(text, noise_level=0.05):
    words = text.split()
    num_noisy_words = max(1, int(len(words) * noise_level))
    for _ in range(num_noisy_words):
        idx = random.randint(0, len(words) - 1)
        words[idx] = words[idx][::-1]
    return " ".join(words)

def load_and_preprocess_data(csv_path):
    df = pd.read_csv(csv_path, names=["english", "french"])
    df.dropna(inplace=True)
    df = df.sample(frac=1).reset_index(drop=True)
    df["english"] = df["english"].apply(lambda x: add_noise(x))
    df["french"] = df["french"].apply(lambda x: add_noise(x))
    return df

def train_cross_encoder(rank, world_size, local_rank, csv_filename, precision="default", epochs=1, batch_size=32, learning_rate=2e-5, model_folder_name="cross_encoder_model"):
    df = load_and_preprocess_data(csv_filename)
    X = list(zip(df["english"], df["french"]))
    y = [1] * len(X)
    split = int(0.9 * len(X))
    X_train, X_val = X[:split], X[split:]
    y_train, y_val = y[:split], y[split:]
    
    precision_modes = ["default", "mixed", "fp16", "bf16", "tf32"] if precision == "all" else [precision]
    
    for prec in precision_modes:
        if rank == 0:
            print(f"Starting training with precision: {prec}")
        
        model = CrossEncoder('google-bert/bert-base-multilingual-uncased', num_labels=1)
        device = torch.device(f"cuda:{local_rank}")
        model.model.to(device)
        
        if prec == "mixed":
            model.model.to(torch.bfloat16)
        elif prec == "fp16":
            model.model.to(torch.float16)
        elif prec == "bf16":
            model.model.to(torch.bfloat16)
        elif prec == "tf32":
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        
        param_dtype = next(model.model.parameters()).dtype
        if rank == 0:
            print(f"Training is happening in precision: {param_dtype}")
        
        train_examples = [InputExample(texts=[sent1, sent2], label=label) for (sent1, sent2), label in zip(X_train, y_train)]
        val_examples = [InputExample(texts=[sent1, sent2], label=label) for (sent1, sent2), label in zip(X_val, y_val)]
        
        train_sampler = DistributedSampler(train_examples, num_replicas=world_size, rank=rank, shuffle=True)
        train_dataloader = DataLoader(train_examples, sampler=train_sampler, batch_size=batch_size)
        
        evaluator = CEBinaryClassificationEvaluator.from_input_examples(val_examples, name="validation")
        warmup_steps = math.ceil(len(train_dataloader) * epochs * 0.1)
        
        start_time = time.time()
        
        for epoch in range(epochs):
            if rank == 0:
                print(f"Epoch {epoch+1}/{epochs}")
            
            model.fit(
                train_dataloader=train_dataloader,
                evaluator=evaluator,
                epochs=1,
                warmup_steps=warmup_steps,
                optimizer_params={'lr': learning_rate},
                save_best_model=True,
                output_path=(model_folder_name + f'_{prec}_best'),
            )
            
            torch.nn.utils.clip_grad_norm_(model.model.parameters(), max_norm=1.0)
            eval_score = evaluator(model)
            
            if rank == 0:
                print(f"Epoch {epoch+1} - Accuracy: {eval_score:.4f}")
        
        training_time = time.time() - start_time
        
        if rank == 0:
            model.save(model_folder_name + f'_{prec}')
            print(f"Model saved at: {model_folder_name}_{prec}")
            print(f"Training Time: {training_time:.2f} seconds")
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train a CrossEncoder model using a CSV dataset with DDP support.",
                                     epilog="Example usage: torchrun --nnodes=1 --nproc_per_node=4 script.py --csv dataset.csv --train fp16")
    parser.add_argument("--train", type=str, choices=["default", "mixed", "fp16", "bf16", "tf32", "all"], default="default", help="Precision mode for training")
    parser.add_argument("--csv", type=str, required=True, help="Path to the input CSV file")
    args = parser.parse_args()
    
    rank, world_size, local_rank = setup_ddp()
    train_cross_encoder(rank, world_size, local_rank, args.csv, precision=args.train)
    cleanup_ddp()
